{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-standard",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"10%\" alt=\"Naas\" src=\"https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-needle",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# FEC - Creer un dashboard PowerBI\n",
    "<a href=\"https://app.naas.ai/user-redirect/naas/downloader?url=https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/FEC/FEC_Creer_un_dashboard_PowerBI.ipynb\" target=\"_parent\"><img src=\"https://img.shields.io/badge/-Open%20in%20Naas-success?labelColor=000000&logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMTAyNHB4IiBoZWlnaHQ9IjEwMjRweCIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgdmVyc2lvbj0iMS4xIj4KIDwhLS0gR2VuZXJhdGVkIGJ5IFBpeGVsbWF0b3IgUHJvIDIuMC41IC0tPgogPGRlZnM+CiAgPHRleHQgaWQ9InN0cmluZyIgdHJhbnNmb3JtPSJtYXRyaXgoMS4wIDAuMCAwLjAgMS4wIDIyOC4wIDU0LjUpIiBmb250LWZhbWlseT0iQ29tZm9ydGFhLVJlZ3VsYXIsIENvbWZvcnRhYSIgZm9udC1zaXplPSI4MDAiIHRleHQtZGVjb3JhdGlvbj0ibm9uZSIgZmlsbD0iI2ZmZmZmZiIgeD0iMS4xOTk5OTk5OTk5OTk5ODg2IiB5PSI3MDUuMCI+bjwvdGV4dD4KIDwvZGVmcz4KIDx1c2UgaWQ9Im4iIHhsaW5rOmhyZWY9IiNzdHJpbmciLz4KPC9zdmc+Cg==\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-library",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T08:30:57.908317Z",
     "iopub.status.busy": "2021-08-17T08:30:57.908010Z",
     "iopub.status.idle": "2021-08-17T08:30:57.920293Z",
     "shell.execute_reply": "2021-08-17T08:30:57.919475Z",
     "shell.execute_reply.started": "2021-08-17T08:30:57.908246Z"
    },
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #fec #powerbi #dataviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-consideration",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "Ce Notebook permet de transformer des fichiers FEC de votre entreprise en un tableau de bord Microsoft Power BI.\n",
    "Le FEC (fichier des écritures comptables) est un export standard des logiciels de comptabilite et une obligation légale en france depuis 2014 afin de déposer ses comptes de manière electronique auprès des services fiscaux.\n",
    "<br><br>\n",
    "-Durée de l’installation = 5 minutes<br>\n",
    "-Support d’installation = [Page Notion](https://www.notion.so/Mode-d-emploi-FECthis-7fc142f2d7ae4a3889fbca28a83acba2/)<br>\n",
    "-Niveau = Facile<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-nightmare",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Alexandre STEVENS](https://www.linkedin.com/in/alexandrestevenspbix/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-laptop",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-beatles",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-pollution",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import naas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-touch",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Lien URL vers le logo de l'entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-peoples",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOGO = \"https://landen.imgix.net/e5hx7wyzf53f/assets/26u7xg7u.png?w=400\"\n",
    "COLOR_1 = None\n",
    "COLOR_2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-driver",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Lire les fichiers FEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-karaoke",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_fec(file_regex,\n",
    "                sep=\",\",\n",
    "                decimal=\".\",\n",
    "                encoding=None,\n",
    "                header=None,\n",
    "                usecols=None,\n",
    "                names=None,\n",
    "                dtype=None):\n",
    "    # Create df init\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Get all files in INPUT_FOLDER\n",
    "    files = [f for f in os.listdir() if re.search(file_regex, f)]\n",
    "    if len(files) == 0:\n",
    "        print(f\"Aucun fichier FEC ne correspond au standard de nomination\")\n",
    "    else:\n",
    "        for file in files:\n",
    "            # Open file and create df\n",
    "            print(file)\n",
    "            tmp_df = pd.read_csv(file,\n",
    "                                 sep=sep,\n",
    "                                 decimal=decimal,\n",
    "                                 encoding=encoding,\n",
    "                                 header=header,\n",
    "                                 usecols=usecols,\n",
    "                                 names=names,\n",
    "                                 dtype=dtype)\n",
    "            # Add filename to df\n",
    "            tmp_df['NOM_FICHIER'] = file\n",
    "\n",
    "            # Concat df\n",
    "            df = pd.concat([df, tmp_df], axis=0, sort=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-brain",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_regex = \"^\\d{9}FEC\\d{8}.txt\"\n",
    "\n",
    "db_init = get_all_fec(file_regex,\n",
    "                      sep='\\t',\n",
    "                      decimal=',',\n",
    "                      encoding='ISO-8859-1',\n",
    "                      header=0)\n",
    "db_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-wallpaper",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-country",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Base de donnée FEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-messenger",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-accident",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_clean = db_init.copy()\n",
    "\n",
    "# Selection des colonnes à conserver\n",
    "to_select = ['NOM_FICHIER',\n",
    "             'EcritureDate',\n",
    "             'CompteNum',\n",
    "             'CompteLib',\n",
    "             'EcritureLib',\n",
    "             'Debit',\n",
    "             'Credit']\n",
    "db_clean = db_clean[to_select]\n",
    "\n",
    "# Renommage des colonnes\n",
    "to_rename = {'EcritureDate': \"DATE\", \n",
    "             'CompteNum': \"COMPTE_NUM\", \n",
    "             'CompteLib': \"RUBRIQUE_N3\", \n",
    "             'EcritureLib': \"RUBRIQUE_N4\", \n",
    "             'Debit': \"DEBIT\", \n",
    "             'Credit': \"CREDIT\"  }\n",
    "db_clean = db_clean.rename(columns=to_rename)\n",
    "\n",
    "#suppression des espaces colonne \"COMPTE_NUM\"\n",
    "db_clean[\"COMPTE_NUM\"] = db_clean[\"COMPTE_NUM\"].astype(str).str.strip()\n",
    "\n",
    "# Mise au format des colonnes\n",
    "db_clean = db_clean.astype({\"NOM_FICHIER\" : str,\n",
    "                            \"DATE\" : str,\n",
    "                            \"COMPTE_NUM\" : str,\n",
    "                            \"RUBRIQUE_N3\" : str,\n",
    "                            \"RUBRIQUE_N4\" : str,\n",
    "                            \"DEBIT\" : float,\n",
    "                            \"CREDIT\" : float,\n",
    "                            })\n",
    "\n",
    "# Mise au format colonne date\n",
    "db_clean[\"DATE\"] = pd.to_datetime(db_clean[\"DATE\"])\n",
    "\n",
    "db_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-chambers",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### Enrichissement de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-industry",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_enr = db_clean.copy()\n",
    "\n",
    "# Ajout colonnes entité et période\n",
    "db_enr['ENTITY'] = db_enr['NOM_FICHIER'].str[:9]\n",
    "db_enr['PERIOD'] = db_enr['NOM_FICHIER'].str[12:-6]\n",
    "db_enr['PERIOD'] = pd.to_datetime(db_enr['PERIOD'], format='%Y%m')\n",
    "db_enr['PERIOD'] = db_enr['PERIOD'].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "# Ajout colonne month et month_index\n",
    "db_enr['MONTH'] = db_enr['DATE'].dt.strftime(\"%b\")\n",
    "db_enr['MONTH_INDEX'] = db_enr['DATE'].dt.month\n",
    "\n",
    "# Calcul de la valeur debit-crédit\n",
    "db_enr[\"VALUE\"] = (db_enr[\"DEBIT\"]) - (db_enr[\"CREDIT\"])\n",
    "\n",
    "db_enr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-supplement",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcul résultat pour équilibrage bilan dans capitaux propre\n",
    "db_rn = db_enr.copy()\n",
    "\n",
    "db_rn = db_rn[db_rn['COMPTE_NUM'].str.contains(r'^6|^7')]\n",
    "\n",
    "to_group = [\"ENTITY\", \"PERIOD\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "db_rn = db_rn.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "db_rn [\"COMPTE_NUM\"] = \"10999999\"\n",
    "db_rn [\"RUBRIQUE_N3\"] = \"RESULTAT\"\n",
    "\n",
    "# Reorganisation colonne\n",
    "to_select = ['ENTITY',\n",
    "             'PERIOD',\n",
    "             'COMPTE_NUM',\n",
    "             'RUBRIQUE_N3',\n",
    "             'VALUE']\n",
    "db_rn = db_rn[to_select]\n",
    "db_rn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-employee",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Base de données FEC aggrégée avec variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-democrat",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### Aggrégation RUBRIQUE N3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-valve",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcul var v = création de dataset avec Period_comp pour merge\n",
    "db_var = db_enr.copy()\n",
    "\n",
    "# Regroupement \n",
    "to_group = [\"ENTITY\",\n",
    "            \"PERIOD\",\n",
    "            \"COMPTE_NUM\",\n",
    "            \"RUBRIQUE_N3\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "db_var = db_var.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Ajout des résultats au dataframe\n",
    "db_var = pd.concat([db_var, db_rn], axis=0, sort=False)\n",
    "\n",
    "# Creation colonne COMP\n",
    "db_var['PERIOD_COMP'] = (db_var['PERIOD'].str[:4].astype(int) - 1).astype(str) + db_var['PERIOD'].str[-3:]\n",
    "\n",
    "db_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-qualification",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### Création de la base comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-modeling",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_comp = db_var.copy()\n",
    "\n",
    "# Suppression de la colonne période\n",
    "db_comp = db_comp.drop(\"PERIOD_COMP\", axis=1)\n",
    "\n",
    "# Renommage des colonnes\n",
    "to_rename = {'VALUE': \"VALUE_N-1\", \n",
    "             'PERIOD': \"PERIOD_COMP\"}\n",
    "db_comp = db_comp.rename(columns=to_rename)\n",
    "\n",
    "db_comp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-warren",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### Jointure des 2 tables et calcul des variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-spine",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Jointure entre les 2 tables\n",
    "join_on = [\"ENTITY\",\n",
    "           \"PERIOD_COMP\",\n",
    "           \"COMPTE_NUM\",\n",
    "           \"RUBRIQUE_N3\"]\n",
    "db_var = pd.merge(db_var, db_comp, how='left', on=join_on).drop(\"PERIOD_COMP\", axis=1).fillna(0)\n",
    "\n",
    "#Création colonne Var V\n",
    "db_var[\"VARV\"] = db_var[\"VALUE\"] - db_var[\"VALUE_N-1\"]\n",
    "\n",
    "#Création colonne Var P (%)\n",
    "db_var[\"VARP\"] = db_var[\"VARV\"] / db_var[\"VALUE_N-1\"]\n",
    "\n",
    "db_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-estonia",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_cat = db_var.copy()\n",
    "\n",
    "# Calcul des rubriques niveau 2\n",
    "def rubrique_N2(row): \n",
    "        numero_compte = str(row[\"COMPTE_NUM\"])\n",
    "        value = float(row[\"VALUE\"])\n",
    "        \n",
    "# BILAN SIMPLIFIE type IFRS NIV2\n",
    "\n",
    "        to_check = [\"^10\", \"^11\", \"^12\", \"^13\", \"^14\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"CAPITAUX_PROPRES\"\n",
    "        \n",
    "        to_check = [\"^15\", \"^16\", \"^17\", \"^18\", \"^19\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"DETTES_FINANCIERES\"\n",
    "        \n",
    "        to_check = [\"^20\", \"^21\", \"^22\", \"^23\", \"^25\", \"^26\", \"^27\", \"^28\", \"^29\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"IMMOBILISATIONS\"\n",
    "        \n",
    "        to_check = [\"^31\", \"^32\", \"^33\", \"^34\", \"^35\", \"^36\", \"^37\", \"^38\", \"^39\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"STOCKS\"\n",
    "        \n",
    "        to_check = [\"^40\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"DETTES_FOURNISSEURS\"\n",
    "        \n",
    "        to_check = [\"^41\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"CREANCES_CLIENTS\"\n",
    "        \n",
    "        to_check = [\"^42\", \"^43\", \"^44\", \"^45\", \"^46\", \"^47\", \"^48\", \"^49\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            if value > 0:\n",
    "                return \"AUTRES_CREANCES\"\n",
    "            else:\n",
    "                return \"AUTRES_DETTES\"\n",
    "        \n",
    "        to_check = [\"^50\", \"^51\", \"^52\", \"^53\", \"^54\", \"^58\", \"^59\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"DISPONIBILITES\"\n",
    "\n",
    "        \n",
    "# COMPTE DE RESULTAT DETAILLE NIV2\n",
    "\n",
    "        to_check = [\"^60\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"ACHATS\"\n",
    "        \n",
    "        to_check= [\"^61\", \"^62\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"SERVICES_EXTERIEURS\"\n",
    "        \n",
    "        to_check = [\"^63\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"TAXES\"        \n",
    "    \n",
    "        to_check = [\"^64\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"CHARGES_PERSONNEL\"    \n",
    "    \n",
    "        to_check = [\"^65\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"AUTRES_CHARGES\"  \n",
    "    \n",
    "        to_check = [\"^66\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"CHARGES_FINANCIERES\"\n",
    "        \n",
    "        to_check = [\"^67\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"CHARGES_EXCEPTIONNELLES\"\n",
    "        \n",
    "        to_check = [\"^68\", \"^78\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"AMORTISSEMENTS\"\n",
    "        \n",
    "        to_check = [\"^69\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"IMPOT\"\n",
    "        \n",
    "        to_check = [\"^70\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"VENTES\"\n",
    "        \n",
    "        to_check = [\"^71\", \"^72\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"PRODUCTION_STOCKEE_IMMOBILISEE\"\n",
    "        \n",
    "        to_check = [\"^74\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"SUBVENTIONS_D'EXPL.\"\n",
    "        \n",
    "        to_check = [\"^75\", \"^791\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"AUTRES_PRODUITS_GESTION_COURANTE\"\n",
    "        \n",
    "        to_check = [\"^76\", \"^796\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"PRODUITS_FINANCIERS\"\n",
    "        \n",
    "        to_check = [\"^77\", \"^797\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"PRODUITS_EXCEPTIONNELS\"\n",
    "        \n",
    "        to_check = [\"^78\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"REPRISES_AMORT._DEP.\"\n",
    "\n",
    "        to_check = [\"^8\"]\n",
    "        if any (re.search(x,numero_compte) for x in to_check):\n",
    "            return \"COMPTES_SPECIAUX\"\n",
    "\n",
    "        \n",
    "# Calcul des rubriques niveau 1\n",
    "def rubrique_N1(row):\n",
    "    categorisation = row.RUBRIQUE_N2\n",
    "    \n",
    "# BILAN SIMPLIFIE type IFRS N1\n",
    "\n",
    "    to_check = [\"CAPITAUX_PROPRES\", \"DETTES_FINANCIERES\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"PASSIF_NON_COURANT\"\n",
    "    \n",
    "    to_check = [\"IMMOBILISATIONS\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"ACTIF_NON_COURANT\"\n",
    "    \n",
    "    to_check = [\"STOCKS\", \"CREANCES_CLIENTS\", \"AUTRES_CREANCES\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"ACTIF_COURANT\"   \n",
    "        \n",
    "    to_check = [\"DETTES_FOURNISSEURS\", \"AUTRES_DETTES\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"PASSIF_COURANT\"\n",
    "    \n",
    "    to_check = [\"DISPONIBILITES\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"DISPONIBILITES\"\n",
    "\n",
    "    \n",
    "# COMPTE DE RESULTAT SIMPLIFIE N1\n",
    "    \n",
    "    to_check = [\"ACHATS\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"COUTS_DIRECTS\"\n",
    "    \n",
    "    to_check = [\"SERVICES_EXTERIEURS\", \"TAXES\", \"CHARGES_PERSONNEL\", \"AUTRES_CHARGES\", \"AMORTISSEMENTS\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"CHARGES_EXPLOITATION\"\n",
    "    \n",
    "    to_check = [\"CHARGES_FINANCIERES\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"CHARGES_FINANCIERES\"\n",
    "        \n",
    "    to_check = [\"CHARGES_EXCEPTIONNELLES\", \"IMPOT\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"CHARGES_EXCEPTIONNELLES\"        \n",
    "\n",
    "    to_check = [\"VENTES\", \"PRODUCTION_STOCKEE_IMMOBILISEE\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"CHIFFRE_D'AFFAIRES\"\n",
    "    \n",
    "    to_check = [\"SUBVENTIONS_D'EXPL.\", \"AUTRES_PRODUITS_GESTION_COURANTE\", \"REPRISES_AMORT._DEP.\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"PRODUITS_EXPLOITATION\"\n",
    "    \n",
    "    to_check = [\"PRODUITS_FINANCIERS\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"PRODUITS_FINANCIERS\"\n",
    "        \n",
    "    to_check = [\"PRODUITS_EXCEPTIONNELS\"]\n",
    "    if any(re.search(x, categorisation) for x in to_check):\n",
    "        return \"PRODUITS_EXCEPTIONNELS\"      \n",
    "        \n",
    "\n",
    "# Calcul des rubriques niveau 0\n",
    "def rubrique_N0(row):\n",
    "    masse = row.RUBRIQUE_N1\n",
    "    \n",
    "    to_check = [\"ACTIF_NON_COURANT\", \"ACTIF_COURANT\", \"DISPONIBILITES\"]\n",
    "    if any(re.search(x, masse) for x in to_check):\n",
    "        return \"ACTIF\"\n",
    "    \n",
    "    to_check = [\"PASSIF_NON_COURANT\", \"PASSIF_COURANT\"]\n",
    "    if any(re.search(x, masse) for x in to_check):\n",
    "        return \"PASSIF\"\n",
    "\n",
    "    to_check = [\"COUTS_DIRECTS\", \"CHARGES_EXPLOITATION\", \"CHARGES_FINANCIERES\", \"CHARGES_EXCEPTIONNELLES\"]\n",
    "    if any(re.search(x, masse) for x in to_check):\n",
    "        return \"CHARGES\"   \n",
    "        \n",
    "    to_check = [\"CHIFFRE_D'AFFAIRES\", \"PRODUITS_EXPLOITATION\", \"PRODUITS_FINANCIERS\", \"PRODUITS_EXCEPTIONNELS\"]\n",
    "    if any(re.search(x, masse) for x in to_check):\n",
    "        return \"PRODUITS\"  \n",
    "    \n",
    "    \n",
    "# Mapping des rubriques \n",
    "db_cat[\"RUBRIQUE_N2\"] = db_cat.apply(lambda row: rubrique_N2(row), axis=1)\n",
    "db_cat[\"RUBRIQUE_N1\"] = db_cat.apply(lambda row: rubrique_N1(row), axis=1)\n",
    "db_cat[\"RUBRIQUE_N0\"] = db_cat.apply(lambda row: rubrique_N0(row), axis=1)    \n",
    "\n",
    "\n",
    "# Reorganisation colonne\n",
    "to_select = ['ENTITY',\n",
    "             'PERIOD',\n",
    "             'COMPTE_NUM', \n",
    "             'RUBRIQUE_N0',\n",
    "             'RUBRIQUE_N1',\n",
    "             'RUBRIQUE_N2',\n",
    "             'RUBRIQUE_N3',\n",
    "             'VALUE',\n",
    "             'VALUE_N-1',\n",
    "             'VARV',\n",
    "             'VARP']\n",
    "db_cat = db_cat[to_select]\n",
    "\n",
    "db_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-superintendent",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Modèles de données des graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-prophet",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### REF_ENTITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-toronto",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du dataset ref_entite\n",
    "dataset_entite = db_cat.copy()\n",
    "\n",
    "# Regrouper par entite\n",
    "to_group = [\"ENTITY\"]\n",
    "to_agg = {\"ENTITY\": \"max\"}\n",
    "dataset_entite = dataset_entite.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Affichage du modèle de donnée\n",
    "dataset_entite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-gates",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### REF_SCENARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-housing",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du dataset ref_scenario\n",
    "dataset_scenario = db_cat.copy()\n",
    "\n",
    "# Regrouper par entite\n",
    "to_group = [\"PERIOD\"]\n",
    "to_agg = {\"PERIOD\": \"max\"}\n",
    "dataset_scenario = dataset_scenario.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Affichage du modèle de donnée\n",
    "dataset_scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-palestine",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### KPIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-buddy",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du dataset KPIS (CA, MARGE, EBE, BFR, CC, DF)\n",
    "dataset_kpis = db_cat.copy()\n",
    "\n",
    "# KPIs CA\n",
    "dataset_kpis_ca = dataset_kpis[dataset_kpis.RUBRIQUE_N1.isin([\"CHIFFRE_D'AFFAIRES\"])]\n",
    "\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N1\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_ca = dataset_kpis_ca.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Passage value postif\n",
    "dataset_kpis_ca[\"VALUE\"] = dataset_kpis_ca[\"VALUE\"]*-1\n",
    "\n",
    "\n",
    "# COUTS_DIRECTS\n",
    "dataset_kpis_ha = dataset_kpis[dataset_kpis.RUBRIQUE_N1.isin([\"COUTS_DIRECTS\"])]\n",
    "\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N1\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_ha = dataset_kpis_ha.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Passage value négatif\n",
    "dataset_kpis_ha[\"VALUE\"] = dataset_kpis_ha[\"VALUE\"]*-1\n",
    "\n",
    "\n",
    "# KPIs MARGE BRUTE (CA - COUTS DIRECTS)\n",
    "dataset_kpis_mb = dataset_kpis_ca.copy()\n",
    "dataset_kpis_mb = pd.concat([dataset_kpis_mb, dataset_kpis_ha], axis=0, sort=False)\n",
    "\n",
    "to_group = [\"ENTITY\",\n",
    "            \"PERIOD\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "\n",
    "dataset_kpis_mb = dataset_kpis_mb.groupby(to_group, as_index=False).agg(to_agg)\n",
    "dataset_kpis_mb[\"RUBRIQUE_N1\"] = \"MARGE\"\n",
    "dataset_kpis_mb = dataset_kpis_mb[[\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N1\", \"VALUE\"]]\n",
    "\n",
    "\n",
    "# CHARGES EXTERNES\n",
    "dataset_kpis_ce = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"SERVICES_EXTERIEURS\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_ce = dataset_kpis_ce.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Passage value negatif\n",
    "dataset_kpis_ce[\"VALUE\"] = dataset_kpis_ce[\"VALUE\"]*-1\n",
    "\n",
    "\n",
    "# IMPOTS\n",
    "dataset_kpis_ip = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"TAXES\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_ip = dataset_kpis_ip.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Passage value negatif\n",
    "dataset_kpis_ip[\"VALUE\"] = dataset_kpis_ip[\"VALUE\"]*-1\n",
    "\n",
    "\n",
    "# CHARGES DE PERSONNEL\n",
    "dataset_kpis_cp = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"CHARGES_PERSONNEL\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_cp = dataset_kpis_cp.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Passage value negatif\n",
    "dataset_kpis_cp[\"VALUE\"] = dataset_kpis_cp[\"VALUE\"]*-1\n",
    "\n",
    "\n",
    "# AUTRES_CHARGES\n",
    "dataset_kpis_ac = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"AUTRES_CHARGES\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_ac = dataset_kpis_ac.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Passage value negatif\n",
    "dataset_kpis_ac[\"VALUE\"] = dataset_kpis_ac[\"VALUE\"]*-1\n",
    "\n",
    "\n",
    "# SUBVENTIONS D'EXPLOITATION\n",
    "dataset_kpis_ac = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"SUBVENTIONS_D'EXPL.\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_ac = dataset_kpis_ac.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "\n",
    "# KPIs EBE = MARGE - CHARGES EXTERNES - TAXES - CHARGES PERSONNEL - AUTRES CHARGES + SUBVENTION D'EXPLOITATION\n",
    "dataset_kpis_ebe = dataset_kpis_mb.copy()\n",
    "dataset_kpis_ebe = pd.concat([dataset_kpis_ebe, dataset_kpis_ce, dataset_kpis_ip, dataset_kpis_cp, dataset_kpis_ac], axis=0, sort=False)\n",
    "\n",
    "to_group = [\"ENTITY\", \"PERIOD\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "\n",
    "dataset_kpis_ebe = dataset_kpis_ebe.groupby(to_group, as_index=False).agg(to_agg)\n",
    "dataset_kpis_ebe[\"RUBRIQUE_N1\"] = \"EBE\"\n",
    "dataset_kpis_ebe = dataset_kpis_ebe[[\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N1\", \"VALUE\"]]\n",
    "\n",
    "\n",
    "# KPIs CREANCES CLIENTS\n",
    "dataset_kpis_cc = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"CREANCES_CLIENTS\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_cc = dataset_kpis_cc.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Renommage colonne\n",
    "to_rename = {'RUBRIQUE_N2': \"RUBRIQUE_N1\"}\n",
    "dataset_kpis_cc = dataset_kpis_cc.rename(columns=to_rename)\n",
    "\n",
    "\n",
    "# KPIs STOCKS\n",
    "dataset_kpis_st = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"STOCKS\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_st = dataset_kpis_st.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Renommage colonne\n",
    "to_rename = {'RUBRIQUE_N2': \"RUBRIQUE_N1\"}\n",
    "dataset_kpis_st = dataset_kpis_st.rename(columns=to_rename)\n",
    "\n",
    "\n",
    "# KPIs DETTES FOURNISSEURS\n",
    "dataset_kpis_df = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([\"DETTES_FOURNISSEURS\"])]\n",
    "to_group = [\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_df = dataset_kpis_df.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Renommage colonne\n",
    "to_rename = {'RUBRIQUE_N2': \"RUBRIQUE_N1\"}\n",
    "dataset_kpis_df = dataset_kpis_df.rename(columns=to_rename)\n",
    "\n",
    "# Passage value positif\n",
    "dataset_kpis_df[\"VALUE\"] = dataset_kpis_df[\"VALUE\"].abs()\n",
    "\n",
    "\n",
    "# KPIs BFR = CREANCES + STOCKS - DETTES FOURNISSEURS\n",
    "dataset_kpis_bfr_df = dataset_kpis_df.copy()\n",
    "\n",
    "# Passage dette fournisseur value négatif\n",
    "dataset_kpis_bfr_df[\"VALUE\"] = dataset_kpis_bfr_df[\"VALUE\"]*-1\n",
    "\n",
    "dataset_kpis_bfr_df = pd.concat([dataset_kpis_cc, dataset_kpis_st, dataset_kpis_bfr_df], axis=0, sort=False)\n",
    "\n",
    "to_group = [\"ENTITY\", \"PERIOD\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_kpis_bfr_df = dataset_kpis_bfr_df.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "# Creation colonne Rubrique_N1 = BFR\n",
    "dataset_kpis_bfr_df[\"RUBRIQUE_N1\"] = \"BFR\"\n",
    "\n",
    "# Reorganisation colonne\n",
    "dataset_kpis_bfr_df = dataset_kpis_bfr_df[[\"ENTITY\", \"PERIOD\", \"RUBRIQUE_N1\", \"VALUE\"]]\n",
    "\n",
    "\n",
    "# Creation du dataset final\n",
    "dataset_kpis_final = pd.concat([dataset_kpis_ca, dataset_kpis_mb, dataset_kpis_ebe, dataset_kpis_cc, dataset_kpis_st, dataset_kpis_df, dataset_kpis_bfr_df], axis=0, sort=False)\n",
    "\n",
    "\n",
    "# Creation colonne COMP\n",
    "dataset_kpis_final['PERIOD_COMP'] = (dataset_kpis_final['PERIOD'].str[:4].astype(int) - 1).astype(str) + dataset_kpis_final['PERIOD'].str[-3:]\n",
    "dataset_kpis_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-grammar",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creation base comparable pour dataset_kpis\n",
    "dataset_kpis_final_comp = dataset_kpis_final.copy()\n",
    "\n",
    "# Suppression de la colonne période\n",
    "dataset_kpis_final_comp = dataset_kpis_final_comp.drop(\"PERIOD_COMP\", axis=1)\n",
    "\n",
    "# Renommage des colonnes\n",
    "to_rename = {'VALUE': \"VALUE_N-1\", \n",
    "             'PERIOD': \"PERIOD_COMP\"}\n",
    "dataset_kpis_final_comp = dataset_kpis_final_comp.rename(columns=to_rename)\n",
    "dataset_kpis_final_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-consultation",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Jointure entre les 2 tables dataset_kpis_final et dataset_kpis_vf\n",
    "join_on = [\"ENTITY\",\n",
    "           \"PERIOD_COMP\",\n",
    "           \"RUBRIQUE_N1\"]\n",
    "dataset_kpis_final = pd.merge(dataset_kpis_final, dataset_kpis_final_comp, how='left', on=join_on).drop(\"PERIOD_COMP\", axis=1).fillna(0)\n",
    "\n",
    "#Création colonne Var V\n",
    "dataset_kpis_final[\"VARV\"] = dataset_kpis_final[\"VALUE\"] - dataset_kpis_final[\"VALUE_N-1\"]\n",
    "\n",
    "#Création colonne Var P (%)\n",
    "dataset_kpis_final[\"VARP\"] = dataset_kpis_final[\"VARV\"] / dataset_kpis_final[\"VALUE_N-1\"]\n",
    "\n",
    "dataset_kpis_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-child",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### EVOLUTION CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-azerbaijan",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du dataset evol_ca\n",
    "dataset_evol_ca = db_enr.copy()\n",
    "\n",
    "# Filtre COMPTE_NUM = Chiffre d'Affaire (RUBRIQUE N1)\n",
    "dataset_evol_ca = dataset_evol_ca[dataset_evol_ca['COMPTE_NUM'].str.contains(r'^70|^71|^72')]\n",
    "\n",
    "# Regroupement \n",
    "to_group = [\"ENTITY\",\n",
    "            \"PERIOD\",\n",
    "            \"MONTH\",\n",
    "            \"MONTH_INDEX\",\n",
    "            \"RUBRIQUE_N3\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_evol_ca = dataset_evol_ca.groupby(to_group, as_index=False).agg(to_agg)\n",
    "\n",
    "dataset_evol_ca[\"VALUE\"] = dataset_evol_ca[\"VALUE\"].abs()\n",
    "\n",
    "\n",
    "# Calcul de la somme cumulée\n",
    "dataset_evol_ca = dataset_evol_ca.sort_values(by=[\"ENTITY\", 'PERIOD', 'MONTH_INDEX']).reset_index(drop=True)\n",
    "dataset_evol_ca['MONTH_INDEX'] = pd.to_datetime(dataset_evol_ca['MONTH_INDEX'], format=\"%m\").dt.strftime(\"%m\")\n",
    "dataset_evol_ca['VALUE_CUM'] = dataset_evol_ca.groupby([\"ENTITY\", \"PERIOD\"], as_index=True).agg({\"VALUE\": \"cumsum\"})\n",
    "\n",
    "# Affichage du modèle de donnée\n",
    "dataset_evol_ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-harvard",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### CHARGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-equilibrium",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creation du dataset charges\n",
    "dataset_charges = db_cat.copy()\n",
    "\n",
    "# Filtre RUBRIQUE_N0 = CHARGES\n",
    "dataset_charges = dataset_charges[dataset_charges[\"RUBRIQUE_N0\"] == \"CHARGES\"]\n",
    "\n",
    "# Mettre en valeur positive VALUE\n",
    "dataset_charges[\"VALUE\"] = dataset_charges[\"VALUE\"].abs()\n",
    "\n",
    "# Affichage du modèle de donnée\n",
    "dataset_charges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-generator",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### POSITIONS TRESORERIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-difficulty",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du dataset trésorerie\n",
    "dataset_treso = db_enr.copy()\n",
    "\n",
    "# Filtre RUBRIQUE_N1 = TRESORERIE\n",
    "dataset_treso = dataset_treso[dataset_treso['COMPTE_NUM'].str.contains(r'^5')].reset_index(drop=True)\n",
    "\n",
    "# Cash in / Cash out ?\n",
    "dataset_treso.loc[dataset_treso.VALUE > 0, \"CASH_IN\"] = dataset_treso.VALUE\n",
    "dataset_treso.loc[dataset_treso.VALUE < 0, \"CASH_OUT\"] = dataset_treso.VALUE\n",
    "\n",
    "# Regroupement \n",
    "to_group = [\"ENTITY\",\n",
    "            \"PERIOD\",\n",
    "            \"MONTH\",\n",
    "            \"MONTH_INDEX\"]\n",
    "to_agg = {\"VALUE\": \"sum\",\n",
    "          \"CASH_IN\": \"sum\",\n",
    "          \"CASH_OUT\": \"sum\"}\n",
    "dataset_treso = dataset_treso.groupby(to_group, as_index = False).agg(to_agg).fillna(0)\n",
    "\n",
    "# Cumul par période\n",
    "dataset_treso = dataset_treso.sort_values([\"ENTITY\", \"PERIOD\", \"MONTH_INDEX\"])\n",
    "dataset_treso['MONTH_INDEX'] = pd.to_datetime(dataset_treso['MONTH_INDEX'], format=\"%m\").dt.strftime(\"%m\")\n",
    "dataset_treso['VALUE_LINE'] = dataset_treso.groupby([\"ENTITY\", 'PERIOD'], as_index=True).agg({\"VALUE\": \"cumsum\"})\n",
    "\n",
    "# Mettre en valeur positive CASH_OUT\n",
    "dataset_treso[\"CASH_OUT\"] = dataset_treso[\"CASH_OUT\"].abs()\n",
    "\n",
    "# Affichage du modèle de donnée\n",
    "dataset_treso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-california",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "#### BILAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-secret",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation du dataset Bilan\n",
    "dataset_bilan = db_cat.copy()\n",
    "\n",
    "# Filtre RUBRIQUE_N0 = ACTIF & PASSIF\n",
    "dataset_bilan = dataset_bilan[(dataset_bilan[\"RUBRIQUE_N0\"].isin([\"ACTIF\", \"PASSIF\"]))]\n",
    "\n",
    "# Regroupement R0/R1/R2\n",
    "to_group = [\"ENTITY\",\n",
    "            \"PERIOD\",\n",
    "            \"RUBRIQUE_N0\",\n",
    "            \"RUBRIQUE_N1\",\n",
    "            \"RUBRIQUE_N2\"]\n",
    "to_agg = {\"VALUE\": \"sum\"}\n",
    "dataset_bilan = dataset_bilan.groupby(to_group, as_index = False).agg(to_agg).fillna(0)\n",
    "\n",
    "\n",
    "# Mettre en valeur positive VALUE\n",
    "dataset_bilan[\"VALUE\"] = dataset_bilan[\"VALUE\"].abs()\n",
    "\n",
    "# Selectionner les colonnes\n",
    "to_select = [\"ENTITY\",\n",
    "             \"PERIOD\",\n",
    "             \"RUBRIQUE_N0\",\n",
    "             \"RUBRIQUE_N1\",\n",
    "             \"RUBRIQUE_N2\",\n",
    "             \"VALUE\"]\n",
    "dataset_bilan = dataset_bilan[to_select]\n",
    "\n",
    "# Affichage du modèle de donnée\n",
    "dataset_bilan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-birthday",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-clone",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Sauvegarde des fichiers en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-story",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_to_csv(df, filename):\n",
    "    # Sauvegarde en csv\n",
    "    df.to_csv(filename,\n",
    "              sep=\";\",\n",
    "              decimal=\",\",\n",
    "              index=False)\n",
    "    \n",
    "    # Création du lien url\n",
    "    naas_link = naas.asset.add(filename)\n",
    "    \n",
    "    # Création de la ligne\n",
    "    data = {\n",
    "        \"OBJET\": filename,\n",
    "        \"URL\": naas_link,\n",
    "        \"DATE_EXTRACT\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    return pd.DataFrame([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-tract",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_logo = {\n",
    "    \"OBJET\": \"Logo\",\n",
    "    \"URL\": LOGO,\n",
    "    \"DATE_EXTRACT\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "logo = pd.DataFrame([dataset_logo])\n",
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-evolution",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "color = {\"name\":\"Color\",\n",
    "         \"dataColors\":[COLOR_1, COLOR_2]}\n",
    "\n",
    "with open(\"color.json\", \"w\") as write_file:\n",
    "    json.dump(color, write_file)\n",
    "\n",
    "dataset_color = {\n",
    "    \"OBJET\": \"Color\",\n",
    "    \"URL\": naas.asset.add(\"color.json\"),\n",
    "    \"DATE_EXTRACT\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "pbi_color = pd.DataFrame([dataset_color])\n",
    "pbi_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-relative",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "entite = df_to_csv(dataset_entite, \"dataset_entite.csv\")\n",
    "entite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-postage",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenario = df_to_csv(dataset_scenario, \"dataset_scenario.csv\")\n",
    "scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-editing",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "kpis = df_to_csv(dataset_kpis_final, \"dataset_kpis_final.csv\")\n",
    "kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-rates",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "evol_ca = df_to_csv(dataset_evol_ca, \"dataset_evol_ca.csv\")\n",
    "evol_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-sequence",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "charges = df_to_csv(dataset_charges, \"dataset_charges.csv\")\n",
    "charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-tampa",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "treso = df_to_csv(dataset_treso, \"dataset_treso.csv\")\n",
    "treso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-textbook",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "bilan = df_to_csv(dataset_bilan, \"dataset_bilan.csv\")\n",
    "bilan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-hypothesis",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Création du fichier à intégrer dans PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-species",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_powerbi = pd.concat([logo, pbi_color, entite, scenario, kpis, evol_ca, charges, treso, bilan], axis=0)\n",
    "db_powerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-aberdeen",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_to_csv(db_powerbi, \"powerbi.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}