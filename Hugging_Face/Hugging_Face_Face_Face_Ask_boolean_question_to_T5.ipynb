{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T06:02:01.679489Z",
     "iopub.status.busy": "2021-05-08T06:02:01.679228Z",
     "iopub.status.idle": "2021-05-08T06:02:01.695397Z",
     "shell.execute_reply": "2021-05-08T06:02:01.694380Z",
     "shell.execute_reply.started": "2021-05-08T06:02:01.679428Z"
    },
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"10%\" alt=\"Naas\" src=\"https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-professor",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Hugging_Face - Hugging Face Face Ask boolean question to T5\n",
    "<a href=\"https://app.naas.ai/user-redirect/naas/downloader?url=https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/Hugging_Face/Hugging_Face_Face_Ask_boolean_question_to_T5.ipynb\" target=\"_parent\"><img src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/open_in_naas.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6fa04-7426-4971-aad3-59c1e903bfe4",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #huggingface #ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bff0b-3d5e-45fe-a010-0058c1a07210",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Jeremy Ravenel](https://www.linkedin.com/in/ACoAAAJHE7sB5OxuKHuzguZ9L6lfDHqw--cdnJg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-plastic",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## T5-base finetuned on BoolQ (superglue task)\n",
    "This notebook is for demonstrating the training and use of the text-to-text-transfer-transformer (better known as T5) on boolean questions (BoolQ). The example use case is a validator indicating if an idea is environmentally friendly. Nearly any question can be passed into the `query` function (see below) as long as a context to a question is given.\n",
    "\n",
    "Author: Maximilian Frank ([script4all.com](//script4all.com)) - Copyleft license\n",
    "\n",
    "Notes:\n",
    "- The model from [huggingface.co/mrm8488/t5-base-finetuned-boolq](//huggingface.co/mrm8488/t5-base-finetuned-boolq) is used in this example as it is an already trained t5-base model on boolean questions (BoolQ task of superglue).\n",
    "- Documentation references on [huggingface.co/transformers/model_doc/t5.html#training](//huggingface.co/transformers/model_doc/t5.html#training), template script on [programming-review.com/machine-learning/t5](//programming-review.com/machine-learning/t5)\n",
    "- The greater the model, the higher the accuracy on BoolQ (see [arxiv.org/pdf/1910.10683.pdf](//arxiv.org/pdf/1910.10683.pdf)):\n",
    "    t5-small|t5-base|t5-large|t5-3B|t5-11B\n",
    "    -|-|-|-|-\n",
    "    76.4%|81.4%|85.4%|89.9%|91.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-chart",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Loading the model\n",
    "If here comes an error, install the packages via `python3 -m pip install ‚Ä¶ --user`.\n",
    "\n",
    "You can also load a T5 plain model (not finetuned). Just replace the following code\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('mrm8488/t5-base-finetuned-boolq')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('mrm8488/t5-base-finetuned-boolq')‚Ä¶\n",
    "```\n",
    "with\n",
    "```python\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "```\n",
    "where `t5-small` is one of the names in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9704803-b904-4fb8-81f2-c97d35c20dff",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-proceeding",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equivalent-double",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:25.351132Z",
     "iopub.status.busy": "2021-05-08T05:17:25.350861Z",
     "iopub.status.idle": "2021-05-08T05:17:27.173947Z",
     "shell.execute_reply": "2021-05-08T05:17:27.173309Z",
     "shell.execute_reply.started": "2021-05-08T05:17:25.351067Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from operator import itemgetter\n",
    "from distutils.util import strtobool\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b55a32-7454-42ce-89ed-7e287a130cce",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "combined-south",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:29.455469Z",
     "iopub.status.busy": "2021-05-08T05:17:29.455239Z",
     "iopub.status.idle": "2021-05-08T05:17:47.738897Z",
     "shell.execute_reply": "2021-05-08T05:17:47.738226Z",
     "shell.execute_reply.started": "2021-05-08T05:17:29.455444Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mrm8488/t5-base-finetuned-boolq')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('mrm8488/t5-base-finetuned-boolq').to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "try:model.parallelize()\n",
    "except:pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-appointment",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Training\n",
    "> **Optional:** You can leave the following out, if you don't have custom datasets. By default the number of training epochs equals 0, so nothing is trained.\n",
    "\n",
    "> **Warning:** This option consumes a lot of runtime and thus *naas.ai* credits. Make sure to have enough credits on your account.\n",
    "\n",
    "For each dataset a stream-opener has to be provided which is readable line by line (e.g. file, database). In the array with key `keys` are all dictionary keys which exist in the jsonl-line. So in this example the first training dataset has the keys `question` for the questions (string),`passage` for the contexts (string) and `answer` for the answers (boolean). Adjust these keys to your dataset.\n",
    "\n",
    "At last you have to adjust the number of epochs to be trained (see comment `# epochs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unable-scotland",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:47.740244Z",
     "iopub.status.busy": "2021-05-08T05:17:47.740027Z",
     "iopub.status.idle": "2021-05-08T05:17:47.751063Z",
     "shell.execute_reply": "2021-05-08T05:17:47.750542Z",
     "shell.execute_reply.started": "2021-05-08T05:17:47.740215Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "srcs = [\n",
    "    { 'stream': lambda:open('boolq/train.jsonl', 'r'),\n",
    "      'keys': ['question', 'passage', 'answer'] },\n",
    "    { 'stream': lambda:open('boolq/dev.jsonl', 'r'),\n",
    "      'keys': ['question', 'passage', 'answer'] },\n",
    "    { 'stream': lambda:open('boolq-nat-perturb/train.jsonl', 'r'),\n",
    "      'keys': ['question', 'passage', 'roberta_hard'] }\n",
    "]\n",
    "model.train()\n",
    "for _ in range(0): # epochs\n",
    "    for src in srcs:\n",
    "        with src['stream']() as s:\n",
    "            for d in s:\n",
    "                q, p, a = itemgetter(src['keys'][0], src['keys'][1], src['keys'][2])(json.loads(d))\n",
    "                tokens = tokenizer('question:'+q+'\\ncontext:'+p, return_tensors='pt')\n",
    "                if len(tokens.input_ids[0]) > model.config.n_positions:\n",
    "                    continue\n",
    "                model(input_ids=tokens.input_ids,\n",
    "                    labels=tokenizer(str(a), return_tensors='pt').input_ids,\n",
    "                    attention_mask=tokens.attention_mask,\n",
    "                    use_cache=True\n",
    "                    ).loss.backward()\n",
    "model.eval(); # ; suppresses long output on jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-analysis",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Define query function\n",
    "As the model is ready, define the querying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forced-calvin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:17:47.752470Z",
     "iopub.status.busy": "2021-05-08T05:17:47.752250Z",
     "iopub.status.idle": "2021-05-08T05:17:47.845461Z",
     "shell.execute_reply": "2021-05-08T05:17:47.844870Z",
     "shell.execute_reply.started": "2021-05-08T05:17:47.752441Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query(q='question', c='context'):\n",
    "    return strtobool(\n",
    "        tokenizer.decode(\n",
    "            token_ids=model.generate(\n",
    "                input_ids=tokenizer.encode('question:'+q+'\\ncontext:'+c, return_tensors='pt')\n",
    "            )[0],\n",
    "        skip_special_tokens=True,\n",
    "        max_length=3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-legislature",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Querying on the task\n",
    "Now the actual task begins: Query the model with your ideas (see list `ideas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "invalid-flood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T05:39:54.534227Z",
     "iopub.status.busy": "2021-05-08T05:39:54.533994Z",
     "iopub.status.idle": "2021-05-08T05:39:59.255031Z",
     "shell.execute_reply": "2021-05-08T05:39:59.254404Z",
     "shell.execute_reply.started": "2021-05-08T05:39:54.534199Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ideas = [ 'The idea is to pollute the air instead of riding the bike.', # should be false\n",
    "              'The idea is to go cycling instead of driving the car.', # should be true\n",
    "              'The idea is to put your trash everywhere.', # should be false\n",
    "              'The idea is to reduce transport distances.', # should be true\n",
    "              'The idea is to put plants on all the roofs.', # should be true\n",
    "              'The idea is to forbid opensource vaccines.', # should be true\n",
    "              'The idea is to go buy an Iphone every five years.', # should be false \n",
    "              'The idea is to walk once every week in the nature.', # should be true  \n",
    "              'The idea is to go buy Green bonds.', # should be true  \n",
    "              'The idea is to go buy fast fashion.', # should be false\n",
    "              'The idea is to buy single-use items.', # should be false\n",
    "              'The idea is to drink plastic bottled water.', # should be false\n",
    "              'The idea is to use import goods.', # should be false\n",
    "              'The idea is to use buy more food than you need.', # should be false\n",
    "              'The idea is to eat a lot of meat.', # should be false\n",
    "              'The idea is to eat less meat.', # should be false\n",
    "              'The idea is to always travel by plane.', # should be false\n",
    "              'The idea is to opensource vaccines.' # should be false\n",
    "             \n",
    "            ]\n",
    "    for idea in ideas:\n",
    "        print('üåè Idea:', idea)\n",
    "        print('\\t‚úÖ Good idea' if query('Is the idea environmentally friendly?', idea) else '\\t‚ùå Bad idea' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}